FROM ollama/ollama:latest

# Install dependencies
RUN apt-get update && apt-get install -y --no-install-recommends \
    wget \
    ca-certificates \
    gnupg \
    curl \
    sudo \
    && rm -rf /var/lib/apt/lists/*

# Install NVIDIA Container Toolkit exactly as specified
RUN curl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey | gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg \
    && curl -s -L https://nvidia.github.io/libnvidia-container/stable/deb/nvidia-container-toolkit.list \
    | sed 's#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g' \
    > /etc/apt/sources.list.d/nvidia-container-toolkit.list \
    && apt-get update \
    && apt-get install -y nvidia-container-toolkit \
    && nvidia-ctk runtime configure --runtime=docker \
    && rm -rf /var/lib/apt/lists/*

# Create directory for Ollama models
RUN mkdir -p /root/.ollama

# Create a script to pull the model and start Ollama
RUN echo '#!/bin/bash\n\
if [ ! -d "/root/.ollama/models/mistral-nemo" ]; then\n\
  echo "Pulling mistral-nemo model..."\n\
  /bin/ollama pull mistral-nemo\n\
fi\n\
\n\
echo "Starting Ollama server..."\n\
exec /bin/ollama serve\n\
' > /usr/local/bin/start-ollama.sh \
&& chmod +x /usr/local/bin/start-ollama.sh

# Set the entrypoint to our custom script
ENTRYPOINT ["/usr/local/bin/start-ollama.sh"]
