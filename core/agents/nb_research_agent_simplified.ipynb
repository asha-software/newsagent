{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simplified Research Agent\n",
    "\n",
    "This notebook implements a simplified version of the research agent that leverages LangChain/LangGraph's built-in tool calling logic while still tracking which tools are used and collecting evidence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "import json\n",
    "from IPython.display import Image, display\n",
    "from dotenv import load_dotenv\n",
    "import importlib\n",
    "import os\n",
    "import sys\n",
    "from langchain_core.messages import AIMessage, BaseMessage, HumanMessage, ToolMessage, SystemMessage\n",
    "from langchain_ollama import ChatOllama\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "from typing import Annotated, TypedDict, List, Dict, Any\n",
    "\n",
    "notebook_dir = os.getcwd()\n",
    "\n",
    "project_root = os.path.abspath(\n",
    "    os.path.join(notebook_dir, \"../..\"))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "import core.tools.registry\n",
    "\n",
    "load_dotenv(os.path.join(os.path.dirname(notebook_dir), '.env'), override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration and Tool Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tools: ['multiply', 'add', 'divide', 'query']\n"
     ]
    }
   ],
   "source": [
    "MODEL = \"mistral-nemo\"\n",
    "TEMPERATURE = 0\n",
    "\n",
    "TOOL_REGISTRY = {\n",
    "    'core.tools.builtins.calculator': ['multiply', 'add', 'divide'],\n",
    "    'core.tools.builtins.wikipedia': ['query']\n",
    "}\n",
    "\n",
    "def import_function(module_name, function_name):\n",
    "    \"\"\"Dynamically imports a function from a module.\"\"\"\n",
    "    try:\n",
    "        module = importlib.import_module(module_name)\n",
    "        function = getattr(module, function_name)\n",
    "        return function\n",
    "    except (ImportError, AttributeError) as e:\n",
    "        print(f\"Error: Could not import function '{function_name}' from module '{module_name}'.\")\n",
    "        print(f\"Exception: {e}\")\n",
    "        return None\n",
    "\n",
    "all_tools = [import_function(module, function) for module,\n",
    "         functions in TOOL_REGISTRY.items() for function in functions]\n",
    "print(f\"Tools: {[tool.__name__ for tool in all_tools]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## State Definition and System Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class State(TypedDict):\n",
    "    messages: Annotated[list[BaseMessage], add_messages]\n",
    "    claim: str\n",
    "    used_tools: List[str]\n",
    "    # {'name': tool name, 'args': {kwargs}, 'result': str}\n",
    "    evidence: list[dict]\n",
    "\n",
    "with open(os.path.join(notebook_dir, 'prompts/research_agent_system_prompt.txt'), 'r') as f:\n",
    "    sys_msg = SystemMessage(content=f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Node Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(state: State):\n",
    "    \"\"\"\n",
    "    Preprocesses state before sending to the assistant for tool routing.\n",
    "    \"\"\"\n",
    "    state['messages'] = [sys_msg, HumanMessage(content=state['claim'])]\n",
    "    state['used_tools'] = []\n",
    "    state['evidence'] = []\n",
    "    return state\n",
    "\n",
    "def assistant(state: State) -> State:\n",
    "    \"\"\"\n",
    "    The main assistant node that processes the claim and decides which tools to use.\n",
    "    This leverages LangChain/LangGraph's built-in tool calling logic.\n",
    "    \"\"\"\n",
    "    llm_with_tools = ChatOllama(\n",
    "        model=MODEL,\n",
    "        temperature=TEMPERATURE,\n",
    "    ).bind_tools(all_tools)\n",
    "    \n",
    "    response = llm_with_tools.invoke(state['messages'])\n",
    "\n",
    "    if hasattr(response, 'tool_calls'):\n",
    "        for tool_call in response.tool_calls:\n",
    "            tool_name = tool_call['name']\n",
    "            if tool_name not in state['used_tools']:\n",
    "                state['used_tools'].append(tool_name)\n",
    "    \n",
    "    return {\"messages\": response}\n",
    "\n",
    "def postprocessing(state: State) -> State:\n",
    "    \"\"\"\n",
    "    Extract evidence from the message history.\n",
    "    \"\"\"\n",
    "    evidence = []\n",
    "    for i in range(len(state['messages'])):\n",
    "        message = state['messages'][i]\n",
    "        if isinstance(message, AIMessage) and hasattr(message, 'tool_calls'):\n",
    "            for tool_call in message.tool_calls:\n",
    "                for j in range(i + 1, len(state['messages'])):\n",
    "                    next_message = state['messages'][j]\n",
    "                    if isinstance(next_message, ToolMessage) and next_message.tool_call_id == tool_call['id']:\n",
    "                        evidence.append({\n",
    "                            'name': tool_call['name'],\n",
    "                            'args': tool_call['args'],\n",
    "                            'result': next_message.content\n",
    "                        })\n",
    "                        break\n",
    "    \n",
    "    state['evidence'] = evidence\n",
    "    return state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not visualize graph: HTTPSConnectionPool(host='mermaid.ink', port=443): Read timed out. (read timeout=10)\n"
     ]
    }
   ],
   "source": [
    "builder = StateGraph(State)\n",
    "\n",
    "builder.add_node(\"preprocessing\", preprocessing)\n",
    "builder.add_node(\"assistant\", assistant)\n",
    "builder.add_node(\"tools\", ToolNode(all_tools))\n",
    "builder.add_node(\"postprocessing\", postprocessing)\n",
    "\n",
    "builder.add_edge(START, \"preprocessing\")\n",
    "builder.add_edge(\"preprocessing\", \"assistant\")\n",
    "builder.add_conditional_edges(\n",
    "    \"assistant\",\n",
    "    tools_condition,\n",
    "    {\n",
    "        \"tools\": \"tools\",\n",
    "        \"__end__\": \"postprocessing\"\n",
    "    }\n",
    ")\n",
    "builder.add_edge(\"tools\", \"assistant\")\n",
    "builder.add_edge(\"postprocessing\", END)\n",
    "\n",
    "agent = builder.compile()\n",
    "\n",
    "try:\n",
    "    display(Image(agent.get_graph().draw_mermaid_png()))\n",
    "except Exception as e:\n",
    "    print(f\"Could not visualize graph: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing with Example Claims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Factual Claim Test:\n",
      "Claim: Albert Einstein developed the theory of relativity\n",
      "Tools Used:\n",
      "  query\n",
      "Evidence:\n",
      "  Tool: query\n",
      "  Args: {'query': 'Albert Einstein theory of relativity'}\n",
      "  Result: No Wikipedia page found for 'Albert Einstein theory of relativity'.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "factual_claim = \"Albert Einstein developed the theory of relativity\"\n",
    "factual_result = agent.invoke({\"claim\": factual_claim})\n",
    "\n",
    "print(\"\\nFactual Claim Test:\")\n",
    "print(f\"Claim: {factual_claim}\")\n",
    "print(\"Tools Used:\")\n",
    "for tool_name in factual_result.get('used_tools', []):\n",
    "    print(f\"  {tool_name}\")\n",
    "\n",
    "print(\"Evidence:\")\n",
    "for evidence in factual_result.get('evidence', []):\n",
    "    print(f\"  Tool: {evidence['name']}\")\n",
    "    print(f\"  Args: {evidence['args']}\")\n",
    "    print(f\"  Result: {evidence['result'][:100]}...\" if len(evidence['result']) > 100 else f\"  Result: {evidence['result']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mathematical Claim Test:\n",
      "Claim: 12 multiplied by 10 equals 120\n",
      "Tools Used:\n",
      "  multiply\n",
      "Evidence:\n",
      "  Tool: multiply\n",
      "  Args: {'a': 12, 'b': 10}\n",
      "  Result: 120\n",
      "\n"
     ]
    }
   ],
   "source": [
    "math_claim = \"12 multiplied by 10 equals 120\"\n",
    "math_result = agent.invoke({\"claim\": math_claim})\n",
    "\n",
    "print(\"\\nMathematical Claim Test:\")\n",
    "print(f\"Claim: {math_claim}\")\n",
    "print(\"Tools Used:\")\n",
    "for tool_name in math_result.get('used_tools', []):\n",
    "    print(f\"  {tool_name}\")\n",
    "\n",
    "print(\"Evidence:\")\n",
    "for evidence in math_result.get('evidence', []):\n",
    "    print(f\"  Tool: {evidence['name']}\")\n",
    "    print(f\"  Args: {evidence['args']}\")\n",
    "    print(f\"  Result: {evidence['result']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mixed Claim Test:\n",
      "Claim: If you multiply the distance from Earth to the Sun (93 million miles) by 2, you get 186 million miles\n",
      "Tools Used:\n",
      "  query\n",
      "  multiply\n",
      "Evidence:\n",
      "  Tool: query\n",
      "  Args: {'query': 'distance from Earth to Sun'}\n",
      "  Result: No Wikipedia page found for 'distance from Earth to Sun'.\n",
      "\n",
      "  Tool: multiply\n",
      "  Args: {'a': 93000000, 'b': 2}\n",
      "  Result: 186000000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mixed_claim = \"If you multiply the distance from Earth to the Sun (93 million miles) by 2, you get 186 million miles\"\n",
    "mixed_result = agent.invoke({\"claim\": mixed_claim})\n",
    "\n",
    "print(\"\\nMixed Claim Test:\")\n",
    "print(f\"Claim: {mixed_claim}\")\n",
    "print(\"Tools Used:\")\n",
    "for tool_name in mixed_result.get('used_tools', []):\n",
    "    print(f\"  {tool_name}\")\n",
    "\n",
    "print(\"Evidence:\")\n",
    "for evidence in mixed_result.get('evidence', []):\n",
    "    print(f\"  Tool: {evidence['name']}\")\n",
    "    print(f\"  Args: {evidence['args']}\")\n",
    "    print(f\"  Result: {evidence['result'][:100]}...\" if len(evidence['result']) > 100 else f\"  Result: {evidence['result']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
