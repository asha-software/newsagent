{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "OPENAI_API_KEY:  ········\n"
     ]
    }
   ],
   "source": [
    "import os, getpass\n",
    "\n",
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "\n",
    "_set_env(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, load the autoreload extension\n",
    "%load_ext autoreload\n",
    "\n",
    "# Set autoreload to automatically reload all modules\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from dotenv import load_dotenv\n",
    "import importlib\n",
    "import os\n",
    "from langchain_core.messages import AIMessage, BaseMessage, HumanMessage, ToolMessage, SystemMessage\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "from typing import Annotated, TypedDict\n",
    "\n",
    "\n",
    "MODEL = \"gpt-3.5-turbo-0125\"\n",
    "TEMPERATURE = 0\n",
    "# load_dotenv('../.env', override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/yihua/harvard/capstone/project/newsagent/core/agents\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tools.calculator import add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tools: ['Calculator']\n"
     ]
    }
   ],
   "source": [
    "TOOL_REGISTRY = {\n",
    "    'tools.calculator': [ 'Calculator'],\n",
    "    # 'tools.wikipedia': ['query']\n",
    "}\n",
    "\n",
    "def import_function(module_name, function_name):\n",
    "    \"\"\"Dynamically imports a function from a module.\n",
    "\n",
    "    Args:\n",
    "        module_name: The name of the module (e.g., \"my_module\").\n",
    "        function_name: The name of the function to import (e.g., \"my_function\").\n",
    "\n",
    "    Returns:\n",
    "        The imported function, or None if the module or function is not found.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        module = importlib.import_module(module_name)\n",
    "        function = getattr(module, function_name)\n",
    "        return function\n",
    "    except (ImportError, AttributeError):\n",
    "        print(f\"Error: Could not import function '{function_name}' from module '{module_name}'.\")\n",
    "        return None\n",
    "\n",
    "tools = [import_function(module, function) for module, functions in TOOL_REGISTRY.items() for function in functions]\n",
    "print(f\"Tools: {[tool.__name__ for tool in tools]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(\n",
    "    model=MODEL, \n",
    "    temperature=TEMPERATURE,\n",
    "    # base_url=\"http://host.docker.internal:11434\", # if running in the studio\n",
    "    ).bind_tools(tools)\n",
    "\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list[BaseMessage], add_messages]\n",
    "    claim: str\n",
    "    evidence: list[dict]  # {'name': tool name, 'args': {kwargs}, 'result': str}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Function must have a docstring if description not provided.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 52\u001b[39m\n\u001b[32m     50\u001b[39m builder.add_node(\u001b[33m\"\u001b[39m\u001b[33mpreprocessing\u001b[39m\u001b[33m\"\u001b[39m, preprocessing)\n\u001b[32m     51\u001b[39m builder.add_node(\u001b[33m\"\u001b[39m\u001b[33massistant\u001b[39m\u001b[33m\"\u001b[39m, assistant)\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m builder.add_node(\u001b[33m\"\u001b[39m\u001b[33mtools\u001b[39m\u001b[33m\"\u001b[39m, \u001b[43mToolNode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtools\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m     53\u001b[39m builder.add_node(\u001b[33m\"\u001b[39m\u001b[33mpostprocessing\u001b[39m\u001b[33m\"\u001b[39m, postprocessing)\n\u001b[32m     55\u001b[39m \u001b[38;5;66;03m# Define edges: these determine how the control flow moves\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/newsagent/lib/python3.11/site-packages/langgraph/prebuilt/tool_node.py:218\u001b[39m, in \u001b[36mToolNode.__init__\u001b[39m\u001b[34m(self, tools, name, tags, handle_tool_errors, messages_key)\u001b[39m\n\u001b[32m    216\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m tool_ \u001b[38;5;129;01min\u001b[39;00m tools:\n\u001b[32m    217\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tool_, BaseTool):\n\u001b[32m--> \u001b[39m\u001b[32m218\u001b[39m         tool_ = \u001b[43mcreate_tool\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtool_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    219\u001b[39m     \u001b[38;5;28mself\u001b[39m.tools_by_name[tool_.name] = tool_\n\u001b[32m    220\u001b[39m     \u001b[38;5;28mself\u001b[39m.tool_to_state_args[tool_.name] = _get_state_args(tool_)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/newsagent/lib/python3.11/site-packages/langchain_core/tools/convert.py:316\u001b[39m, in \u001b[36mtool\u001b[39m\u001b[34m(name_or_callable, runnable, description, return_direct, args_schema, infer_schema, response_format, parse_docstring, error_on_invalid_docstring, *args)\u001b[39m\n\u001b[32m    310\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m name_or_callable \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    311\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(name_or_callable) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(name_or_callable, \u001b[33m\"\u001b[39m\u001b[33m__name__\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    312\u001b[39m         \u001b[38;5;66;03m# Used as a decorator without parameters\u001b[39;00m\n\u001b[32m    313\u001b[39m         \u001b[38;5;66;03m# @tool\u001b[39;00m\n\u001b[32m    314\u001b[39m         \u001b[38;5;66;03m# def my_tool():\u001b[39;00m\n\u001b[32m    315\u001b[39m         \u001b[38;5;66;03m#    pass\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m316\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_create_tool_factory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_callable\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_callable\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    317\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(name_or_callable, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    318\u001b[39m         \u001b[38;5;66;03m# Used with a new name for the tool\u001b[39;00m\n\u001b[32m    319\u001b[39m         \u001b[38;5;66;03m# @tool(\"search\")\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    326\u001b[39m         \u001b[38;5;66;03m# def my_tool():\u001b[39;00m\n\u001b[32m    327\u001b[39m         \u001b[38;5;66;03m#    pass\u001b[39;00m\n\u001b[32m    328\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m _create_tool_factory(name_or_callable)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/newsagent/lib/python3.11/site-packages/langchain_core/tools/convert.py:261\u001b[39m, in \u001b[36mtool.<locals>._create_tool_factory.<locals>._tool_factory\u001b[39m\u001b[34m(dec_func)\u001b[39m\n\u001b[32m    258\u001b[39m     schema = args_schema\n\u001b[32m    260\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m infer_schema \u001b[38;5;129;01mor\u001b[39;00m args_schema \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m261\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mStructuredTool\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    262\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    263\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcoroutine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    264\u001b[39m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtool_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    265\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdescription\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtool_description\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    266\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_direct\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_direct\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    267\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs_schema\u001b[49m\u001b[43m=\u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    268\u001b[39m \u001b[43m        \u001b[49m\u001b[43minfer_schema\u001b[49m\u001b[43m=\u001b[49m\u001b[43minfer_schema\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    269\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    270\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparse_docstring\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparse_docstring\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    271\u001b[39m \u001b[43m        \u001b[49m\u001b[43merror_on_invalid_docstring\u001b[49m\u001b[43m=\u001b[49m\u001b[43merror_on_invalid_docstring\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    272\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    273\u001b[39m \u001b[38;5;66;03m# If someone doesn't want a schema applied, we must treat it as\u001b[39;00m\n\u001b[32m    274\u001b[39m \u001b[38;5;66;03m# a simple string->string function\u001b[39;00m\n\u001b[32m    275\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m dec_func.\u001b[34m__doc__\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/newsagent/lib/python3.11/site-packages/langchain_core/tools/structured.py:207\u001b[39m, in \u001b[36mStructuredTool.from_function\u001b[39m\u001b[34m(cls, func, coroutine, name, description, return_direct, args_schema, infer_schema, response_format, parse_docstring, error_on_invalid_docstring, **kwargs)\u001b[39m\n\u001b[32m    205\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m description_ \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    206\u001b[39m     msg = \u001b[33m\"\u001b[39m\u001b[33mFunction must have a docstring if description not provided.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m207\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[32m    208\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m description \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    209\u001b[39m     \u001b[38;5;66;03m# Only apply if using the function's docstring\u001b[39;00m\n\u001b[32m    210\u001b[39m     description_ = textwrap.dedent(description_).strip()\n",
      "\u001b[31mValueError\u001b[39m: Function must have a docstring if description not provided."
     ]
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "\n",
    "\n",
    "with open('prompts/research_agent_system_prompt.txt', 'r') as f:\n",
    "    sys_msg = SystemMessage(content=f.read())\n",
    "\n",
    "def preprocessing(state: State):\n",
    "    \"\"\"\n",
    "    Preprocesses state before sending to the assistant for tool routing.\n",
    "    Currently, this just extracts the claim from the state and sets it as a HumanMessage\n",
    "    following the SystemMessage\n",
    "    \"\"\"\n",
    "    return {\"messages\": HumanMessage(content=state['claim'])}\n",
    "\n",
    "def assistant(state: State) -> State:\n",
    "    response = llm.invoke(state['messages'])\n",
    "    return {\"messages\": response}\n",
    "\n",
    "def postprocessing(state: State) -> State:\n",
    "    \"\"\"\n",
    "    Scan the message history to extract tool calls and results into tuples:\n",
    "    (tool_name, tool_args, tool_result) for the 'evidence' list in the state\n",
    "    \"\"\"\n",
    "    \n",
    "    evidence = []\n",
    "    for i in range(len(state['messages'])):\n",
    "        message = state['messages'][i]\n",
    "        if isinstance(message, AIMessage) and hasattr(message, 'tool_calls'):\n",
    "            for tool_call in message.tool_calls:\n",
    "                # Scan later messages for the corresponding ToolMessage\n",
    "                for j in range(i + 1, len(state['messages'])):\n",
    "                    next_message = state['messages'][j]\n",
    "                    if isinstance(next_message, ToolMessage) and next_message.tool_call_id == tool_call['id']:\n",
    "                        # Found the corresponding ToolMessage\n",
    "                        evidence.append({\n",
    "                            'name': tool_call['name'], \n",
    "                            'args': tool_call['args'], \n",
    "                            'result': next_message.content})\n",
    "                        break\n",
    "\n",
    "    return {'evidence': evidence}\n",
    "    # return state\n",
    "\n",
    "\n",
    "# Graph\n",
    "builder = StateGraph(State)\n",
    "\n",
    "# Define nodes: these do the work\n",
    "builder.add_node(\"preprocessing\", preprocessing)\n",
    "builder.add_node(\"assistant\", assistant)\n",
    "builder.add_node(\"tools\", ToolNode(tools))\n",
    "builder.add_node(\"postprocessing\", postprocessing)\n",
    "\n",
    "# Define edges: these determine how the control flow moves\n",
    "builder.add_edge(START, \"preprocessing\")\n",
    "builder.add_edge(\"preprocessing\", \"assistant\")\n",
    "builder.add_conditional_edges(\n",
    "    source=\"assistant\",\n",
    "    path=tools_condition,\n",
    "    path_map={'tools': 'tools', '__end__': 'postprocessing'}\n",
    ")\n",
    "builder.add_edge(\"tools\", \"assistant\")\n",
    "builder.add_edge(\"postprocessing\", END)\n",
    "\n",
    "react_graph = builder.compile()\n",
    "\n",
    "# Show\n",
    "# display(Image(react_graph.get_graph(xray=False).draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_core.messages import HumanMessage, SystemMessage\n",
    "claim = \"1/3 is bigger than 1/4.\"\n",
    "# initial_state = {\"claim\": claim}\n",
    "# final_state = graph.invoke(initial_state)\n",
    "\n",
    "messages = [sys_msg]\n",
    "messages = react_graph.invoke({\"messages\": messages, \"claim\": claim})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m System Message \u001b[0m================================\n",
      "\n",
      "You are an **Evidence Retrieval Agent**. Your role is to retrieve supporting or contradictory evidence for a given claim using the tools available to you.  \n",
      "\n",
      "### **Rules:**  \n",
      "1. **Do Not Evaluate the Claim** – Your task is to retrieve evidence, not to judge the claim's truth.  \n",
      "2. **Select and Use Tools Strategically** – Based on the claim, think carefully which tools would help, and construct the correct tool calls to retrieve relevant evidence.  \n",
      "3. **Return Only Tool Messages** – Your output should consist solely of tool calls with properly structured arguments. Do not generate natural language explanations.  \n",
      "4. **Handle Ambiguity Thoughtfully** – If the claim is unclear, attempt a best-guess search or request clarification through a tool call if applicable.  \n",
      "5. **Retrieve Evidence from Multiple Perspectives** – If conflicting evidence exists, return both supporting and contradictory sources.  \n",
      "\n",
      "### **Behavior:**  \n",
      "- Use available tools efficiently to retrieve the most relevant information.  \n",
      "- Use numeric tools such as calculators for numeric claims\n",
      "- Do not summarize, interpret, or filter the evidence—simply retrieve and present it through tool calls.  \n",
      "\n",
      "Your sole function is to query tools and return raw evidence. You do not form conclusions or engage in reasoning about the claim.\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "1/3 is bigger than 1/4.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  query (e5d05507-9826-4647-9c62-0485a24e75c5)\n",
      " Call ID: e5d05507-9826-4647-9c62-0485a24e75c5\n",
      "  Args:\n",
      "    query: Compare the sizes of fractions 1/3 and 1/4.\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: query\n",
      "\n",
      "No Wikipedia page found for 'Compare the sizes of fractions 1/3 and 1/4.'.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "To compare the sizes of two fractions, we can convert them to decimal form or find a common denominator.\n",
      "\n",
      "For 1/3:\n",
      "1 ÷ 3 = 0.333...\n",
      "\n",
      "For 1/4:\n",
      "1 ÷ 4 = 0.25\n",
      "\n",
      "Comparing these decimals, we see that 0.333... is greater than 0.25. Therefore, 1/3 is bigger than 1/4.\n"
     ]
    }
   ],
   "source": [
    "for m in messages['messages']:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'claim': '1/3 is bigger than 1/4.',\n",
      " 'evidence': [{'args': {'query': 'Compare the sizes of fractions 1/3 and 1/4.'},\n",
      "               'name': 'query',\n",
      "               'result': \"No Wikipedia page found for 'Compare the sizes of \"\n",
      "                         \"fractions 1/3 and 1/4.'.\"}],\n",
      " 'messages': [SystemMessage(content=\"You are an **Evidence Retrieval Agent**. Your role is to retrieve supporting or contradictory evidence for a given claim using the tools available to you.  \\n\\n### **Rules:**  \\n1. **Do Not Evaluate the Claim** – Your task is to retrieve evidence, not to judge the claim's truth.  \\n2. **Select and Use Tools Strategically** – Based on the claim, think carefully which tools would help, and construct the correct tool calls to retrieve relevant evidence.  \\n3. **Return Only Tool Messages** – Your output should consist solely of tool calls with properly structured arguments. Do not generate natural language explanations.  \\n4. **Handle Ambiguity Thoughtfully** – If the claim is unclear, attempt a best-guess search or request clarification through a tool call if applicable.  \\n5. **Retrieve Evidence from Multiple Perspectives** – If conflicting evidence exists, return both supporting and contradictory sources.  \\n\\n### **Behavior:**  \\n- Use available tools efficiently to retrieve the most relevant information.  \\n- Use numeric tools such as calculators for numeric claims\\n- Do not summarize, interpret, or filter the evidence—simply retrieve and present it through tool calls.  \\n\\nYour sole function is to query tools and return raw evidence. You do not form conclusions or engage in reasoning about the claim.\", additional_kwargs={}, response_metadata={}, id='370548b9-e170-4163-9fbd-1ed011e734be'),\n",
      "              HumanMessage(content='1/3 is bigger than 1/4.', additional_kwargs={}, response_metadata={}, id='d042aa24-ad9b-4339-b5af-c69f002cbe21'),\n",
      "              AIMessage(content='', additional_kwargs={}, response_metadata={'model': 'mistral-nemo', 'created_at': '2025-03-15T18:17:03.328178Z', 'done': True, 'done_reason': 'stop', 'total_duration': 2344843000, 'load_duration': 27614750, 'prompt_eval_count': 547, 'prompt_eval_duration': 1509000000, 'eval_count': 33, 'eval_duration': 805000000, 'message': Message(role='assistant', content='', images=None, tool_calls=None)}, id='run-c85ce358-e334-4aee-8ee5-fa0c2e43f2d6-0', tool_calls=[{'name': 'query', 'args': {'query': 'Compare the sizes of fractions 1/3 and 1/4.'}, 'id': 'e5d05507-9826-4647-9c62-0485a24e75c5', 'type': 'tool_call'}], usage_metadata={'input_tokens': 547, 'output_tokens': 33, 'total_tokens': 580}),\n",
      "              ToolMessage(content=\"No Wikipedia page found for 'Compare the sizes of fractions 1/3 and 1/4.'.\", name='query', id='dd7e1db8-9f8d-412e-96d6-bf28b3aec30b', tool_call_id='e5d05507-9826-4647-9c62-0485a24e75c5'),\n",
      "              AIMessage(content='To compare the sizes of two fractions, we can convert them to decimal form or find a common denominator.\\n\\nFor 1/3:\\n1 ÷ 3 = 0.333...\\n\\nFor 1/4:\\n1 ÷ 4 = 0.25\\n\\nComparing these decimals, we see that 0.333... is greater than 0.25. Therefore, 1/3 is bigger than 1/4.', additional_kwargs={}, response_metadata={'model': 'mistral-nemo', 'created_at': '2025-03-15T18:17:06.11906Z', 'done': True, 'done_reason': 'stop', 'total_duration': 2590695208, 'load_duration': 31516291, 'prompt_eval_count': 76, 'prompt_eval_duration': 252000000, 'eval_count': 98, 'eval_duration': 2305000000, 'message': Message(role='assistant', content='To compare the sizes of two fractions, we can convert them to decimal form or find a common denominator.\\n\\nFor 1/3:\\n1 ÷ 3 = 0.333...\\n\\nFor 1/4:\\n1 ÷ 4 = 0.25\\n\\nComparing these decimals, we see that 0.333... is greater than 0.25. Therefore, 1/3 is bigger than 1/4.', images=None, tool_calls=None)}, id='run-34f137c7-c60a-48e3-ab5f-1b3494deeebc-0', usage_metadata={'input_tokens': 76, 'output_tokens': 98, 'total_tokens': 174})]}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "pprint(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ToolMessage(content=\"No Wikipedia page found for 'Compare the sizes of fractions 1/3 and 1/4.'.\", name='query', id='dd7e1db8-9f8d-412e-96d6-bf28b3aec30b', tool_call_id='e5d05507-9826-4647-9c62-0485a24e75c5')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = react_graph.nodes['assistant'].invoke(\n",
    "    {\"messages\": [HumanMessage(content=\"hi there\")]}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': AIMessage(content=\"Hello! How can I help you today? Let's have a nice conversation. If you need any calculations, I can do that too! 😊\", additional_kwargs={}, response_metadata={'model': 'mistral-nemo', 'created_at': '2025-03-15T20:58:08.533286Z', 'done': True, 'done_reason': 'stop', 'total_duration': 4515353959, 'load_duration': 617177417, 'prompt_eval_count': 263, 'prompt_eval_duration': 3137000000, 'eval_count': 32, 'eval_duration': 759000000, 'message': Message(role='assistant', content=\"Hello! How can I help you today? Let's have a nice conversation. If you need any calculations, I can do that too! 😊\", images=None, tool_calls=None)}, id='run-5b14021e-fbe7-4626-ba5c-aee5d59ba529-0', usage_metadata={'input_tokens': 263, 'output_tokens': 32, 'total_tokens': 295})}\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
